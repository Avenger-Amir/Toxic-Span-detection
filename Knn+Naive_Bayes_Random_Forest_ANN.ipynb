{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the name of Allah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model, Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from collections import Counter\n",
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Another</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>violent</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>and</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #        Word Tag\n",
       "0  Sentence: 1     Another   A\n",
       "1  Sentence: 1     violent   P\n",
       "2  Sentence: 1         and   P\n",
       "3  Sentence: 1  aggressive   P\n",
       "4  Sentence: 1   immigrant   P"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('crf_file_real.csv', encoding = \"ISO-8859-1\")\n",
    "# df = df[:10000]\n",
    "df = df.fillna(method='ffill')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "oneDEmbeddings={}\n",
    "oneDTotal=[]\n",
    "def extractEmbeddings():\n",
    "    with open('glove.6B.50d.txt',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coeffs = np.asarray(values[1:],dtype='float32')\n",
    "            #print(word)\n",
    "            #print(coeffs)\n",
    "            embeddings[word] = coeffs\n",
    "            oneDTotal.append(coeffs)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(word):\n",
    "    a=np.zeros(50)\n",
    "    if word not in embeddings:\n",
    "        return a\n",
    "    return embeddings[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings2(word):\n",
    "    a=0\n",
    "    if word not in oneDEmbeddings:\n",
    "        return a\n",
    "    return oneDEmbeddings[word][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(df,neg,pos):\n",
    "    n,m=df.shape\n",
    "    for i in range(0,n):\n",
    "        word=df.iloc[i,1]\n",
    "        label=df.iloc[i,2]\n",
    "        wordEmbedding=getEmbeddings2(word)\n",
    "        if label=='A':\n",
    "            neg.append(wordEmbedding)\n",
    "        else:\n",
    "            pos.append(wordEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(embeddings))\n",
    "# print(type(embeddings))\n",
    "# print(embeddings[\"because\"])\n",
    "# print(oneDTotal[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def reduce_dimension(x):\n",
    "    pca = PCA(n_components=1)\n",
    "    z= pca.fit_transform(x)\n",
    "    return z\n",
    "# principalDf = pd.DataFrame(data = principalComponents\n",
    "#              , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneDTotal=reduce_dimension(oneDTotal)\n",
    "# print(oneDTotal[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for key in embeddings.keys():\n",
    "    oneDEmbeddings[key]=oneDTotal[i]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg=[]\n",
    "pos=[]\n",
    "store(df,neg,pos)\n",
    "pos.sort()\n",
    "neg.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pos.pkl', 'wb') as f:\n",
    "    pickle.dump(pos, f)\n",
    "    \n",
    "with open('neg.pkl', 'wb') as f:\n",
    "    pickle.dump(neg,f)\n",
    "    \n",
    "a_file = open(\"oneDEmbeddings.pkl\", \"wb\")\n",
    "pickle.dump(oneDEmbeddings, a_file)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(\"embeddings.pkl\", \"wb\")\n",
    "pickle.dump(embeddings, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274071\n",
      "20590\n"
     ]
    }
   ],
   "source": [
    "print(len(neg))\n",
    "print(len(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range (0,10):\n",
    "#     print(pos[i])\n",
    "#     print(neg[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSentences(X,Y,df):\n",
    "        n,m=df.shape\n",
    "        tmpX=[]\n",
    "        tmpY=[]\n",
    "        for i in range(0,n):\n",
    "            if i==0 or df.iloc[i,0]!=df.iloc[i-1,0]:\n",
    "                if len(tmpX)>1:\n",
    "                    tmpX.pop()\n",
    "                    tmpY.pop()\n",
    "                    X.append(tmpX)\n",
    "                    Y.append(tmpY)\n",
    "                    tmpX=[]\n",
    "                    tmpY=[]\n",
    "            tmpX.append(df.iloc[i,1])\n",
    "            tmpY.append(df.iloc[i,2])\n",
    "        if len(tmpX)>1:\n",
    "            tmpX.pop()\n",
    "            tmpY.pop()\n",
    "            X.append(tmpX)\n",
    "            Y.append(tmpY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=[],[]\n",
    "extractSentences(X,Y,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(wordEmbedding,pos):\n",
    "    p1=pos\n",
    "    p1=p1.reshape(-1)\n",
    "    w1=wordEmbedding\n",
    "    w1=w1.reshape(-1)\n",
    "    if norm(p1)==0 or norm(w1)==0:\n",
    "        return 0\n",
    "    val=dot(w1,p1)/(norm(w1)*norm(p1))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKNearest(wordEmbedding,pos,tmp,k):\n",
    "    v=[]\n",
    "    for i in range(0,len(pos)):\n",
    "        v.append(cossim(wordEmbedding,pos[i]))\n",
    "        \n",
    "    v.sort(reverse=True)\n",
    "    for i in range(0,k):\n",
    "        tmp.append(v[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKNearest2(oneDwordEmbedding,pos,tmp,k):\n",
    "    low,high=0,len(pos)-1\n",
    "    ans=0\n",
    "    while low<=high:\n",
    "        mid=int((low+high)/2)\n",
    "        if pos[mid]==oneDwordEmbedding:\n",
    "            ans=mid\n",
    "            break\n",
    "        elif pos[mid]<oneDwordEmbedding:\n",
    "            ans=mid\n",
    "            low=mid+1\n",
    "        else:\n",
    "            high=mid-1\n",
    "        \n",
    "    cnt,i,j=0,ans-1,ans+1\n",
    "    while cnt<10 and i>=0 and j<len(pos):\n",
    "        cnt+=1\n",
    "        if abs(pos[i]-oneDwordEmbedding)<=abs(pos[j]-oneDwordEmbedding):\n",
    "            tmp.append(pos[i])\n",
    "            i-=1\n",
    "        else:\n",
    "            tmp.append(pos[j])\n",
    "            j+=1\n",
    "                \n",
    "        while cnt<10 and j<len(pos):\n",
    "            cnt+=1\n",
    "            tmp.append(pos[j])\n",
    "            j+=1\n",
    "            \n",
    "        while cnt<10 and i>=0:\n",
    "            cnt+=1\n",
    "            tmp.append(pos[i])\n",
    "            i-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_dataset2(dataset,X,Y,pos,neg,k1):\n",
    "    n=len(X)\n",
    "    for i in range(0,n):\n",
    "#         print(i,n)\n",
    "#         if i==1:\n",
    "#             break\n",
    "        for j in range(0,len(X[i])):\n",
    "            tmp=[]\n",
    "            word=X[i][j].lower()\n",
    "            label=0 if Y[i][j]=='A' else 1\n",
    "            wordEmbedding=getEmbeddings(word)\n",
    "            oneDwordEmbedding=getEmbeddings2(word)\n",
    "            #print(word,wordEmbedding,label)\n",
    "            getKNearest2(oneDwordEmbedding,pos,tmp,k1)\n",
    "#             print(tmp)\n",
    "#             print(\"amir\")\n",
    "            getKNearest2(oneDwordEmbedding,neg,tmp,k1)\n",
    "#             print(tmp)\n",
    "            for k in range(j-1,max(-1,j-6),-1):\n",
    "                l=0 if Y[i][k]=='A' else 1\n",
    "                newEmbedding=getEmbeddings(X[i][k].lower())\n",
    "                tmp.append(cossim(wordEmbedding,newEmbedding))\n",
    "                tmp.append(l)\n",
    "            while len(tmp)<2*k1+10:\n",
    "                tmp.append(0)\n",
    "            tmp.append(label)\n",
    "            #print(len(tmp),word)\n",
    "            dataset.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_dataset(dataset,X,Y,pos,neg,k1):\n",
    "    n=len(X)\n",
    "    for i in range(0,n):\n",
    "#         print(i,n)\n",
    "        if i==10:\n",
    "            break\n",
    "        for j in range(0,len(X[i])):\n",
    "            tmp=[]\n",
    "            word=X[i][j].lower()\n",
    "            label=0 if Y[i][j]=='A' else 1\n",
    "            wordEmbedding=getEmbeddings(word)\n",
    "            #print(word,wordEmbedding,label)\n",
    "            getKNearest(wordEmbedding,pos,tmp,k1)\n",
    "            getKNearest(wordEmbedding,neg,tmp,k1)\n",
    "            for k in range(j-1,max(-1,j-6),-1):\n",
    "                l=0 if Y[i][k]=='A' else 1\n",
    "                newEmbedding=getEmbeddings(X[i][k].lower())\n",
    "                tmp.append(cossim(wordEmbedding,newEmbedding))\n",
    "                tmp.append(l)\n",
    "            while len(tmp)<2*k1+10:\n",
    "                tmp.append(0)\n",
    "            tmp.append(label)\n",
    "            #print(len(tmp),word)\n",
    "            dataset.append(tmp)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "store_dataset2(dataset,X,Y,pos,neg,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,3):\n",
    "#     print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFile(fileName,data):\n",
    "    #savetxt('data.csv', data, delimiter=',')\n",
    "    df=pd.DataFrame(data)\n",
    "    df.to_csv(fileName+\".csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveFile(\"XEmbedded\",XEmbedded)\n",
    "saveFile(\"ANN_Dataset\",dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "df.head(50)\n",
    "print(type(dataset))\n",
    "dataset=np.array(dataset)\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=dataset[:,:-1]\n",
    "y_train=dataset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286722\n",
      "286722\n",
      "Counter({0.0: 266132, 1.0: 20590})\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(Y_train))\n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "#plot_2d_space(X_resampled, y_resampled, 'SMOTE over-sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532264\n",
      "532264\n",
      "Counter({0.0: 266132, 1.0: 266132})\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_real.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "    \n",
    "with open('y_train_real.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_real.pkl', 'rb') as f:\n",
    "        X_train = pickle.load(f)\n",
    "with open('y_train_real.pkl', 'rb') as f:\n",
    "    y_train_real = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3,random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes :- Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7973990417522245\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_bayes_model.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle in a file\n",
    "joblib.dump(gnb, 'naive_bayes_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest :- Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9582477754962354\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle in a file\n",
    "joblib.dump(clf, 'random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural network :- Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first neural network with keras tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=30, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imblearn\n",
    "#!pip install delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)\n",
    "#plot_2d_space(X_resampled, y_resampled, 'SMOTE over-sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1295/1295 [==============================] - 15s 9ms/step - loss: 0.5790 - accuracy: 0.6895 - f1: 0.6280\n",
      "Epoch 2/10\n",
      "1295/1295 [==============================] - 8s 6ms/step - loss: 0.3993 - accuracy: 0.8189 - f1: 0.7984\n",
      "Epoch 3/10\n",
      "1295/1295 [==============================] - 9s 7ms/step - loss: 0.3917 - accuracy: 0.8298 - f1: 0.8176\n",
      "Epoch 4/10\n",
      "1295/1295 [==============================] - 8s 6ms/step - loss: 0.3948 - accuracy: 0.8292 - f1: 0.8180\n",
      "Epoch 5/10\n",
      "1295/1295 [==============================] - 8s 6ms/step - loss: 0.3917 - accuracy: 0.8296 - f1: 0.8177\n",
      "Epoch 6/10\n",
      "1295/1295 [==============================] - 9s 7ms/step - loss: 0.3829 - accuracy: 0.8348 - f1: 0.8239\n",
      "Epoch 7/10\n",
      "1295/1295 [==============================] - 10s 8ms/step - loss: 0.3866 - accuracy: 0.8296 - f1: 0.8239\n",
      "Epoch 8/10\n",
      "1295/1295 [==============================] - 11s 8ms/step - loss: 0.3777 - accuracy: 0.8361 - f1: 0.8260\n",
      "Epoch 9/10\n",
      "1295/1295 [==============================] - 9s 7ms/step - loss: 0.3798 - accuracy: 0.8349 - f1: 0.8281\n",
      "Epoch 10/10\n",
      "1295/1295 [==============================] - 8s 6ms/step - loss: 0.3794 - accuracy: 0.8339 - f1: 0.8259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4807250950>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X_resampled, y_resampled, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 1s 7ms/step - loss: 0.4761 - accuracy: 0.7645 - f1: 0.2630\n",
      "Accuracy: 76.45 0.26\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy,f1_score = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f %.2f' % (accuracy*100,f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ann_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"ann_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ann_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tmp=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "print(len(tmp))\n",
    "model.predict([tmp])\n",
    "print(type(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
